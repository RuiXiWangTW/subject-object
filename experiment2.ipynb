{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Su-Z77SQJp"
      },
      "source": [
        "This colab trains a subject-object classifier probe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG6O747exBax",
        "outputId": "41b09822-a0f9-455d-ea00-8a2e52f5119d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mU37De9-FO3",
        "outputId": "0bf88ee8-9a46-4d19-cbfa-34f163af1217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing create_dataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile create_dataset.py\n",
        "\n",
        "\"\"\"\n",
        "Run one iteration of the experiment, training on one language and testing on another.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sys\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "from utils import get_tokens_and_labels, get_tokens_and_labels_csv, get_bert_tokens, shuffle_positions, save_sample, save_bert_outputs, save_just_position_word\n",
        "\n",
        "base_path = \"content\"\n",
        "def __main__():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--ud-path', type=str,\n",
        "        default=None)\n",
        "    parser.add_argument('--csv-file', type=str, default=None, help=\"If data is in a csv file with subjects and objects marked\")\n",
        "    parser.add_argument('--bert-name', type=str, help=\"Like 'bert-base-uncased'\")\n",
        "    parser.add_argument('--shuffle-positions', action=\"store_true\")\n",
        "    parser.add_argument('--single-position', type=int, default=-1,\n",
        "        help=\"Make all positions this one index\")\n",
        "    parser.add_argument('--local-shuffle', type=int, default=-1)\n",
        "    args = parser.parse_args()\n",
        "    print(\"args:\", args)\n",
        "    make_dataset(args)\n",
        "\n",
        "def make_dataset(args):\n",
        "    if args.ud_path is not None:\n",
        "        tb_name = os.path.split(args.ud_path)[1]\n",
        "        tb_name = os.path.splitext(tb_name)[0]\n",
        "        directory = os.path.join(base_path, f\"{tb_name}_{args.bert_name}\")\n",
        "    if args.csv_file is not None:\n",
        "        dataset_name = os.path.split(args.csv_file)[1]\n",
        "        dataset_name = os.path.splitext(dataset_name)[0]\n",
        "        directory = os.path.join(base_path, f\"{dataset_name}_{args.bert_name}\")\n",
        "    if args.single_position >= 0:\n",
        "        directory += f\"_singlepos{args.single_position}\"\n",
        "    elif args.shuffle_positions:\n",
        "        directory += f\"_shuffled-pos\"\n",
        "    elif args.local_shuffle >= 0:\n",
        "        directory += f\"_localshuffle{args.local_shuffle}\"\n",
        "    #os.mkdir(directory)\n",
        "    directory = \"/content\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.bert_name)\n",
        "    model = AutoModel.from_pretrained(args.bert_name, output_hidden_states=True)\n",
        "    model.eval()\n",
        "    if args.ud_path is not None:\n",
        "        labels = get_tokens_and_labels(args.ud_path)\n",
        "    elif args.csv_file is not None:\n",
        "        labels = get_tokens_and_labels_csv(args.csv_file)\n",
        "    labels = shuffle_positions(labels, args.shuffle_positions, args.local_shuffle)\n",
        "    json.dump(labels, open(os.path.join(directory, \"labels.json\"), \"w\"))\n",
        "    save_sample(20, labels, directory)\n",
        "    bert_info = {}\n",
        "    bert_info[\"bert_tokens\"], bert_info[\"bert_ids\"], \\\n",
        "    bert_info[\"orig_to_bert_map\"], bert_info[\"bert_to_orig_map\"] =\\\n",
        "        get_bert_tokens(labels[\"token\"], tokenizer)\n",
        "    pickle.dump(bert_info, open(os.path.join(directory, \"bert_info.pkl\"), \"wb\"))\n",
        "    bert_vectors_path = os.path.join(directory, \"bert_vectors.hdf5\")\n",
        "    save_bert_outputs(directory, bert_info[\"bert_ids\"], model, args.shuffle_positions, args.single_position)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    __main__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF9PIWaILs0C",
        "outputId": "e71f62ce-b6d6-4520-bd7b-c18f519b632d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing create_index.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile create_index.py\n",
        "\n",
        "import argparse\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "from random import shuffle\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from utils import load_embeddings\n",
        "\n",
        "base_path = \"/content\"\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dataset', type=str)\n",
        "    parser.add_argument('--roles', nargs='+', type=str, help=\"Roles, like A or S-passive. Should be capitalized correctly\")\n",
        "    parser.add_argument('--cases', nargs='+', type=str, help=\"Cases, like Erg or Nom. Should be capitalized correctly\")\n",
        "    parser.add_argument('--balance', action=\"store_true\")\n",
        "    parser.add_argument('--only-non-prototypical', action=\"store_true\")\n",
        "    parser.add_argument('--limit', type=int, default=-1)\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    create_index(args)\n",
        "\n",
        "def create_index(args):\n",
        "    directory = \"/content\" #os.path.join(base_path, args.dataset)\n",
        "    labels = json.load(open(os.path.join(directory, \"labels.json\"), \"r\"))\n",
        "    index = []\n",
        "    if args.roles:\n",
        "        role_index = dict([(role, []) for role in args.roles])\n",
        "    else:\n",
        "        role_index = defaultdict(list)\n",
        "\n",
        "    plain_index_name = \"index\"\n",
        "    if args.balance:\n",
        "        plain_index_name += \"_balance\"\n",
        "    if args.roles:\n",
        "        plain_index_name += \"_roles-\" + \"\".join(args.roles)\n",
        "    if args.cases:\n",
        "        plain_index_name += \"_cases-\" + \"\".join(args.cases)\n",
        "    if args.limit > 0:\n",
        "        plain_index_name += f\"limit-{args.limit}\"\n",
        "\n",
        "    if args.only_non_prototypical:\n",
        "        dataset_directory = os.path.join(\"dataset_storing\", args.dataset)\n",
        "        word_embeddings = load_embeddings(dataset_directory, \"word_embeddings\")\n",
        "        orig_to_bert_map = \\\n",
        "            pickle.load(open(os.path.join(dataset_directory, \"bert_info.pkl\"), \"rb\"))[\"orig_to_bert_map\"]\n",
        "        classifier_dir = os.path.join(\"classifiers\", args.dataset, plain_index_name)\n",
        "        classifier, labelset, labeldict = pickle.load(\n",
        "            open(os.path.join(classifier_dir, f\"mlp_layer-word_embeddings\"), \"rb\"))\n",
        "        A_index = labeldict[\"A\"]\n",
        "        filename = plain_index_name + \"only-non-prototypical.json\"\n",
        "    else:\n",
        "        filename = plain_index_name + \".json\"\n",
        "\n",
        "    for sent_i in range(len(labels[\"token\"])):\n",
        "        for word_i in range(len(labels[\"token\"][sent_i])):\n",
        "            role = labels[\"role\"][sent_i][word_i]\n",
        "            case = labels[\"case\"][sent_i][word_i] if \"case\" in labels else None\n",
        "            role_ok = args.roles is None or role in args.roles\n",
        "            role_ok = role_ok and role is not None\n",
        "            case_ok = args.cases is None or case in args.cases\n",
        "            if role_ok and case_ok:\n",
        "                if not args.only_non_prototypical or \\\n",
        "                   check_non_prototypical(sent_i, word_i, word_embeddings, orig_to_bert_map, classifier, A_index, role):\n",
        "                    role_index[role].append((sent_i, word_i))\n",
        "    if args.balance:\n",
        "        min_role_len = min([len(role_index[role]) for role in role_index.keys()])\n",
        "        if args.limit > 0:\n",
        "            if min_role_len * len(role_index.keys()) >= args.limit:\n",
        "                min_role_len = args.limit // len(role_index.keys())\n",
        "            else:\n",
        "                print(f\"Please pick a limit which is less than the balanced length. Limit = {args.limit}, min_role_len = {min_role_len} for roles {role_index.keys()}\")\n",
        "                sys.exit(1)\n",
        "        print(f\"Culling all roles to have length {min_role_len}\")\n",
        "        for role in role_index.keys():\n",
        "            shuffle(role_index[role])\n",
        "            index.extend(role_index[role][:min_role_len])\n",
        "    else:\n",
        "        if args.limit > 0:\n",
        "            print(f\"Limit not implemented for unbalanced index yet!\")\n",
        "            sys.exit(1)\n",
        "        for role in role_index.keys():\n",
        "            index.extend(role_index[role])\n",
        "    json.dump(index, open(os.path.join(directory, filename), \"w\"))\n",
        "    print(\"Index has length\", len(index))\n",
        "\n",
        "def check_non_prototypical(sent_i, word_i, word_embeddings, orig_to_bert_map, classifier, A_index, role):\n",
        "    bert_start_index = orig_to_bert_map[sent_i][word_i]\n",
        "    word_embedding = word_embeddings[sent_i].squeeze()[bert_start_index]\n",
        "    classifier_output = classifier(torch.Tensor(word_embedding))\n",
        "    probs = torch.softmax(classifier_output, 0)\n",
        "    A_prob = probs[A_index].item()\n",
        "    if role == \"A\" and A_prob < 0.5:\n",
        "        print(\"A\", A_prob)\n",
        "        return True\n",
        "    elif role == \"O\" and A_prob > 0.5:\n",
        "        print(\"O\", A_prob)\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4Xzt8h5YmXv",
        "outputId": "7dc07aa4-917d-43fc-d114-d56584893d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_classifiers.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_classifiers.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from data import SimpleDataset\n",
        "from utils import get_num_layers, train_classifier\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dataset-name', type=str)\n",
        "    parser.add_argument('--index-name', type=str)\n",
        "    parser.add_argument(\"--classifier-type\", type=str, default=\"mlp\")\n",
        "    args = parser.parse_args()\n",
        "    print(\"args\", args)\n",
        "\n",
        "    train_classifiers(args)\n",
        "\n",
        "def train_classifiers(args):\n",
        "    classifier_dir = os.path.join(\"classifiers\", args.dataset_name, args.index_name)\n",
        "    print(\"making classifier dir at\", classifier_dir)\n",
        "    if not os.path.exists(classifier_dir):\n",
        "      os.makedirs(classifier_dir)\n",
        "    num_layers = get_num_layers(args.dataset_name)\n",
        "    print(f\"There are {num_layers} layers in this model\")\n",
        "    layers = [\"word_embeddings\"] +  [str(i) for i in range(num_layers + 1)]\n",
        "    logistic = args.classifier_type == \"logistic\"\n",
        "    for layer in layers:\n",
        "        print(f\"Layer {layer}\")\n",
        "        dataset = SimpleDataset(args.dataset_name, args.index_name, layer)\n",
        "        classifier = train_classifier(dataset, logistic)\n",
        "        pickle.dump((classifier, dataset.labelset, dataset.labeldict),\n",
        "            open(os.path.join(classifier_dir, f\"{args.classifier_type}_layer-{layer}\"), \"wb\"))\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGAHNR3_rIcE",
        "outputId": "25a8be25-5a51-488f-a49d-f2bdafde4b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "from collections import defaultdict\n",
        "import conllu\n",
        "import csv\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel\n",
        "\n",
        "def get_tokens_and_labels(filename):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    filename: te location of the treebank (conll file)\n",
        "\n",
        "    This function parses the conll file to get:\n",
        "    - labels: A dict, whose keys are types of labels (eg, \"animacy\"), and each\n",
        "        value is a list of length num_sentences\n",
        "    \"\"\"\n",
        "    with open(filename) as f:\n",
        "        conll_data = f.read()\n",
        "    sentences = conllu.parse(conll_data)\n",
        "    labels = defaultdict(list)\n",
        "    num_nouns = 0\n",
        "    num_relevant_examples = 0\n",
        "    for sent_i, tokenlist in enumerate(sentences):\n",
        "        sentence_info = defaultdict(list)\n",
        "        if \"sent_id\" in tokenlist.metadata.keys():\n",
        "            sentence_info[\"sent_id\"] = [tokenlist.metadata[\"sent_id\"]]*len(tokenlist)\n",
        "        noun_count = 0\n",
        "        for token in tokenlist:\n",
        "            token_info = get_token_info(token, tokenlist)\n",
        "            token_case = None\n",
        "            token_animacy = \"\"\n",
        "            if token_info[\"role\"] is not None:\n",
        "                if token['feats'] and 'Case' in token['feats']:\n",
        "                    token_case = token['feats']['Case']\n",
        "                if token['feats'] and 'Animacy' in token['feats']:\n",
        "                    token_animacy = token['feats']['Animacy']\n",
        "            token_info[\"case\"] = token_case\n",
        "            token_info[\"animacy\"] = token_animacy\n",
        "            sentence_info[\"token\"].append(token['form'])\n",
        "            for label_type in token_info.keys():\n",
        "                sentence_info[label_type].append(token_info[label_type])\n",
        "            sentence_info[\"preceding_nouns\"].append(noun_count)\n",
        "            if token[\"upostag\"] == \"NOUN\" or token[\"upostag\"] == \"PROPN\" or token[\"upostag\"]==\"PRON\":\n",
        "                noun_count += 1\n",
        "        for label_type in sentence_info.keys():\n",
        "            labels[label_type].append(sentence_info[label_type])\n",
        "        labels[\"word_index\"].append(list(range(len(sentence_info[\"token\"]))))\n",
        "        assert len(sentence_info[\"case\"]) == len(sentence_info[\"role\"]), \\\n",
        "               \"Length of case and role should be the same for every sentence (though both lists can include Nones)\"\n",
        "    print(\"returning from get_tokens, the keys are\", list(labels.keys()))\n",
        "    return dict(labels)\n",
        "\n",
        "def get_tokens_and_labels_csv(filename):\n",
        "    labels = defaultdict(list)\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile, delimiter=',')\n",
        "        for row in reader:\n",
        "            sentence = row['sentence'].split(\" \")\n",
        "            labels[\"token\"].append(sentence)\n",
        "            labels['sent_id'].append([row['sentence_id']]*len(sentence))\n",
        "            subject_idx = int(row[\"subject_idx\"])\n",
        "            object_idx = int(row[\"object_idx\"])\n",
        "            roles = [None]*len(sentence)\n",
        "            roles[subject_idx] = \"A\"\n",
        "            roles[object_idx] = \"O\"\n",
        "            labels[\"role\"].append(roles)\n",
        "            subject_words = [None]*len(sentence)\n",
        "            subject_words[subject_idx] = row[\"subject\"]\n",
        "            subject_words[object_idx] = row[\"subject\"]\n",
        "            labels[\"subject_word\"].append(subject_words)\n",
        "            object_words = [None]*len(sentence)\n",
        "            object_words[subject_idx] = row[\"object\"]\n",
        "            object_words[object_idx] = row[\"object\"]\n",
        "            labels[\"object_word\"].append(object_words)\n",
        "            verb_words = [None]*len(sentence)\n",
        "            verb_words[subject_idx] = row[\"verb\"]\n",
        "            verb_words[object_idx] = row[\"verb\"]\n",
        "            labels[\"verb_word\"].append(verb_words)\n",
        "            labels[\"word_index\"].append(list(range(len(sentence))))\n",
        "    return labels\n",
        "\n",
        "def get_token_info(token, tokenlist):\n",
        "    token_info = {}\n",
        "    token_info[\"role\"] = None\n",
        "    token_info[\"verb_word\"] = \"\"\n",
        "    token_info[\"verb_idx\"] = -1\n",
        "    token_info[\"subject_word\"] = \"\"\n",
        "    token_info[\"object_word\"] = \"\"\n",
        "    if not (token[\"upostag\"] == \"NOUN\" or token[\"upostag\"] == \"PROPN\"):\n",
        "        return token_info\n",
        "\n",
        "    head_id = token['head']\n",
        "    head_list = tokenlist.filter(id=head_id)\n",
        "    head_pos = None\n",
        "    if len(head_list) > 0:\n",
        "        head_token = head_list[0]\n",
        "        if head_token[\"upostag\"] == \"VERB\":\n",
        "            head_pos = \"verb\"\n",
        "            token_info[\"verb_word\"] = head_token[\"lemma\"]\n",
        "            token_info[\"verb_idx\"] = int(head_token[\"id\"]) - 1\n",
        "        elif head_token[\"upostag\"] == \"AUX\":\n",
        "            head_pos = \"aux\"\n",
        "            token_info[\"verb_word\"] = head_token[\"lemma\"]\n",
        "            token_info[\"verb_idx\"] = int(head_token[\"id\"]) - 1\n",
        "        else:\n",
        "            return token_info\n",
        "\n",
        "    if \"nsubj\" in token['deprel']:\n",
        "        token_info[\"subject_word\"] = token['form']\n",
        "        has_object = False\n",
        "        has_expletive_sibling = False\n",
        "        # 'deps' field is often empty in treebanks, have to look through\n",
        "        # the whole sentence to find if there is any object of the head\n",
        "        # verb of this subject (this would determine if it's an A or an S)\n",
        "        for obj_token in tokenlist:\n",
        "            if obj_token['head'] == head_id:\n",
        "                if \"obj\" in obj_token['deprel']:\n",
        "                    has_object = True\n",
        "                    token_info[\"object_word\"] = obj_token[\"form\"]\n",
        "                if obj_token['deprel'] == \"expl\":\n",
        "                    has_expletive_sibling = True\n",
        "        if has_expletive_sibling:\n",
        "            token_info[\"role\"] = \"S-expletive\"\n",
        "        elif has_object:\n",
        "            token_info[\"role\"] = \"A\"\n",
        "        else:\n",
        "            token_info[\"role\"] = \"S\"\n",
        "        if \"pass\" in token['deprel']:\n",
        "            token_info[\"role\"] += \"-passive\"\n",
        "    elif \"obj\" in token['deprel']:\n",
        "        token_info[\"role\"] = \"O\"\n",
        "        token_info[\"object_word\"] = token['form']\n",
        "        for subj_token in tokenlist:\n",
        "            if subj_token['head'] == head_id:\n",
        "                if \"subj\" in subj_token['deprel']:\n",
        "                    token_info[\"subject_word\"] = subj_token['form']\n",
        "    if head_pos == \"aux\" and token_info[\"role\"] is not None:\n",
        "        token_info[\"role\"] += \"-aux\"\n",
        "    return token_info\n",
        "\n",
        "def get_bert_tokens(orig_tokens, tokenizer):\n",
        "    \"\"\"\n",
        "    Given a list of sentences, return a list of those sentences in BERT tokens,\n",
        "    and a list mapping between the indices of each sentence, where\n",
        "    bert_tokens_map[i][j] tells us where in the list bert_tokens[i] to find the\n",
        "    start of the word in sentence_list[i][j]\n",
        "    The input orig_tokens should be a list of lists, where each element is a word.\n",
        "    \"\"\"\n",
        "    bert_tokens = []\n",
        "    orig_to_bert_map = []\n",
        "    bert_to_orig_map = []\n",
        "    for i, sentence in enumerate(orig_tokens):\n",
        "        sentence_bert_tokens = []\n",
        "        sentence_map_otb = []\n",
        "        sentence_map_bto = []\n",
        "        sentence_bert_tokens.append(\"[CLS]\")\n",
        "        for orig_idx, orig_token in enumerate(sentence):\n",
        "            sentence_map_otb.append(len(sentence_bert_tokens))\n",
        "            tokenized = tokenizer.tokenize(orig_token)\n",
        "            for bert_token in tokenized:\n",
        "                sentence_map_bto.append(orig_idx)\n",
        "            sentence_bert_tokens.extend(tokenizer.tokenize(orig_token))\n",
        "        sentence_map_otb.append(len(sentence_bert_tokens))\n",
        "        sentence_bert_tokens = sentence_bert_tokens[:511]\n",
        "        sentence_bert_tokens.append(\"[SEP]\")\n",
        "        bert_tokens.append(sentence_bert_tokens)\n",
        "        orig_to_bert_map.append(sentence_map_otb)\n",
        "        bert_to_orig_map.append(sentence_map_bto)\n",
        "    bert_ids = [tokenizer.convert_tokens_to_ids(b) for b in bert_tokens]\n",
        "    return bert_tokens, bert_ids, orig_to_bert_map, bert_to_orig_map\n",
        "\n",
        "def shuffle_positions(labels, shuffle_positions, local_shuffle):\n",
        "    if not shuffle_positions and local_shuffle <= 0:\n",
        "        print(\"Not shuffling positions this time\")\n",
        "        return labels\n",
        "    assert not shuffle_positions or local_shuffle <= 0, \\\n",
        "        \"Must choose between local and global shuffling!\"\n",
        "    labels[\"shuffled_index\"] = []\n",
        "\n",
        "    for sent_i, sentence in enumerate(labels[\"token\"]):\n",
        "        length = len(sentence)\n",
        "        if shuffle_positions:\n",
        "            permutation = list(range(length))\n",
        "            random.shuffle(permutation)\n",
        "        elif local_shuffle > 0:\n",
        "            permutation = list(range(length))\n",
        "            for chunk_start in range(0, length, local_shuffle):\n",
        "                chunk_end = min(chunk_start + local_shuffle, length)\n",
        "                chunk = permutation[chunk_start:chunk_end]\n",
        "                random.shuffle(chunk)\n",
        "                permutation[chunk_start:chunk_end] = chunk\n",
        "        for label in labels:\n",
        "            if label is not \"shuffled_index\":\n",
        "                labels[label][sent_i] = \\\n",
        "                    [labels[label][sent_i][permutation[word_i]] for word_i in range(length)]\n",
        "        labels[\"shuffled_index\"].append(list(range(length)))\n",
        "    return labels\n",
        "\n",
        "def save_sample(num_samples, labels, directory):\n",
        "    samples = []\n",
        "    sampled_sentences = random.sample(range(len(labels['token'])), num_samples)\n",
        "    for sent_i in sampled_sentences:\n",
        "        sentence = \" \".join(labels[\"token\"][sent_i])\n",
        "        sentence_id = labels[\"sent_id\"][sent_i][0]\n",
        "        samples.append([sentence_id, sentence])\n",
        "    with open(os.path.join(directory, \"sample.csv\"), \"w\") as f:\n",
        "        writer = csv.writer(f, delimiter='\\t')\n",
        "        writer.writerow([\"sentence_id\", \"sentence\"])\n",
        "        writer.writerows(samples)\n",
        "\n",
        "def save_bert_outputs(directory, bert_ids, bert_model, shuffle_positions=False, single_position=-1):\n",
        "    \"\"\"\n",
        "    Given a list of lists of bert IDs, runs them through BERT.\n",
        "    Cache the results to hdf5_path, and load them from there if available.\n",
        "    \"\"\"\n",
        "    assert not shuffle_positions or not single_position >= 0, \\\n",
        "        \"Choose beetween shuffling and putting a single position\"\n",
        "\n",
        "    datafile = h5py.File(os.path.join(directory, \"bert_vectors.hdf5\"), 'w')\n",
        "    word_file = h5py.File(os.path.join(directory, \"bert_word_embs.hdf5\"), 'w')\n",
        "    position_file = h5py.File(os.path.join(directory, \"bert_position_embs.hdf5\"), 'w')\n",
        "    with torch.no_grad():\n",
        "        print(f\"Running {len(bert_ids)} sentences through BERT. This takes a while\")\n",
        "        for idx, sentence in enumerate(tqdm(bert_ids)):\n",
        "            if single_position >= 0:\n",
        "                positions = torch.ones((1, len(sentence)), dtype=torch.long) * single_position\n",
        "            else:\n",
        "                positions = torch.tensor(range(len(sentence))).unsqueeze(0)\n",
        "\n",
        "            bert_output = bert_model(torch.tensor(sentence).unsqueeze(0),\n",
        "                                     position_ids = positions)\n",
        "            hidden_layers = bert_output[\"hidden_states\"]\n",
        "            layer_count = len(hidden_layers)\n",
        "            _, sentence_length, dim = hidden_layers[0].shape\n",
        "            dset = datafile.create_dataset(str(idx), (layer_count, sentence_length, dim))\n",
        "            dset[:, :, :] = np.vstack([np.array(x) for x in hidden_layers])\n",
        "\n",
        "            word_embedding = bert_model.embeddings.word_embeddings(torch.tensor(sentence))\n",
        "            sentence_length, dim = word_embedding.shape\n",
        "            word_dset = word_file.create_dataset(str(idx), (sentence_length, dim))\n",
        "            word_dset[:,:] = word_embedding\n",
        "\n",
        "            position_embedding = bert_model.embeddings.position_embeddings(positions)\n",
        "            pos_dset = position_file.create_dataset(str(idx), (sentence_length, dim))\n",
        "            pos_dset[:,:] = position_embedding\n",
        "    datafile.close()\n",
        "    word_file.close()\n",
        "    position_file.close()\n",
        "\n",
        "def save_just_position_word(directory, bert_ids, bert_model, shuffle_positions=False, single_position=-1):\n",
        "    \"\"\"\n",
        "    NOTE: NOT USED. save_bert_outputs includes this functionality.\n",
        "    Given a list of lists of bert IDs, runs them through BERT.\n",
        "    Cache the results to hdf5_path, and load them from there if available.\n",
        "    \"\"\"\n",
        "    assert not shuffle_positions or not single_position >= 0, \\\n",
        "        \"Choose beetween shuffling and putting a single position\"\n",
        "\n",
        "    word_file = h5py.File(os.path.join(directory, \"bert_word_embs.hdf5\"), 'w')\n",
        "    position_file = h5py.File(os.path.join(directory, \"bert_position_embs.hdf5\"), 'w')\n",
        "    with torch.no_grad():\n",
        "        print(f\"Running {len(bert_ids)} sentences through BERT. This takes a while\")\n",
        "        for idx, sentence in enumerate(tqdm(bert_ids)):\n",
        "            if single_position >= 0:\n",
        "                positions = torch.ones((1, len(sentence)), dtype=torch.long) * single_position\n",
        "            elif shuffle_positions:\n",
        "                # Shuffle positions of everything except for first and last BERT tokens.\n",
        "                positions = torch.arange(len(sentence), dtype=torch.long)\n",
        "                permutation = torch.randperm(len(sentence)-2)\n",
        "                positions[1:-1] = positions[1:-1][permutation]\n",
        "                positions = positions.unsqueeze(0)\n",
        "            else:\n",
        "                positions = torch.tensor(range(len(sentence))).unsqueeze(0)\n",
        "\n",
        "            word_embedding = bert_model.embeddings.word_embeddings(torch.tensor(sentence))\n",
        "            sentence_length, dim = word_embedding.shape\n",
        "            word_dset = word_file.create_dataset(str(idx), (sentence_length, dim))\n",
        "            word_dset[:,:] = word_embedding\n",
        "\n",
        "            position_embedding = bert_model.embeddings.position_embeddings(positions)\n",
        "            pos_dset = position_file.create_dataset(str(idx), (sentence_length, dim))\n",
        "            pos_dset[:,:] = position_embedding\n",
        "    word_file.close()\n",
        "    position_file.close()\n",
        "\n",
        "def load_bert_outputs(directory, layer):\n",
        "    hdf5_path = os.path.join(directory, \"bert_vectors.hdf5\")\n",
        "    try:\n",
        "        layer = int(layer)\n",
        "    except:\n",
        "        print(\"Please use a valid layer in 0-12. If you want word embeddings, use the get_word_embeddings method\")\n",
        "    outputs = []\n",
        "    try:\n",
        "        with h5py.File(hdf5_path, 'r') as datafile:\n",
        "            max_key = max([int(key) for key in datafile.keys()])\n",
        "            for i in tqdm(range(max_key + 1), desc='[Loading from disk]'):\n",
        "                hidden_layers = datafile[str(i)[:]]\n",
        "                output = np.array(hidden_layers[layer])\n",
        "                outputs.append(output)\n",
        "            print(f\"Loaded {i} sentences from disk.\")\n",
        "    except OSError:\n",
        "        print(f\"Encountered hdf5 reading error on file {hdf5_path}. Please re-create the hdf5 file\")\n",
        "        sys.exit(1)\n",
        "    return outputs\n",
        "\n",
        "def load_embeddings(directory, embeddings_type):\n",
        "    if embeddings_type == \"word_embeddings\":\n",
        "        hdf5_path = os.path.join(directory, \"bert_word_embs.hdf5\")\n",
        "    elif embeddings_type == \"position_embeddings\":\n",
        "        hdf5_path = os.path.join(directory, \"bert_position_embs.hdf5\")\n",
        "    else:\n",
        "        print(embeddings_type, \"Is not not word_embeddings or position_embeddings\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    outputs = []\n",
        "    try:\n",
        "        with h5py.File(hdf5_path, 'r') as datafile:\n",
        "            max_key = max([int(key) for key in datafile.keys()])\n",
        "            for i in tqdm(range(max_key + 1), desc='[Loading from disk]'):\n",
        "                outputs.append(np.array(datafile[str(i)][:]))\n",
        "    except OSError:\n",
        "        print(f\"Encountered hdf5 reading error on file {hdf5_path}. Please re-create the hdf5 file\")\n",
        "        sys.exit(1)\n",
        "    return outputs\n",
        "\n",
        "def get_num_layers(dataset_name):\n",
        "    #dataset_directory = os.path.join(\"dataset_storing\", dataset_name)\n",
        "    dataset_directory = \"/content\"\n",
        "    hdf5_path = os.path.join(dataset_directory, \"bert_vectors.hdf5\")\n",
        "    try:\n",
        "        with h5py.File(hdf5_path, 'r') as datafile:\n",
        "            hidden_layers = datafile[str(0)[:]]\n",
        "            return hidden_layers.shape[0]\n",
        "    except OSError:\n",
        "        print(f\"Encountered hdf5 reading error on file {hdf5_path}. Please re-create the hdf5 file\")\n",
        "        sys.exit(1)\n",
        "\n",
        "class _classifier(nn.Module):\n",
        "    def __init__(self, nlabel, bert_dim):\n",
        "        super(_classifier, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(bert_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, nlabel),\n",
        "            nn.Dropout(.1)\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "def train_classifier(dataset, logistic):\n",
        "    if logistic:\n",
        "        return train_classifier_logistic(dataset)\n",
        "    else:\n",
        "        return train_classifier_mlp(dataset)\n",
        "\n",
        "def train_classifier_mlp(train_dataset, epochs=20):\n",
        "    classifier = _classifier(train_dataset.get_num_labels(), train_dataset.get_bert_dim())\n",
        "    optimizer = torch.optim.Adam(classifier.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    dataloader = train_dataset.get_dataloader()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        losses = []\n",
        "        for emb_batch, role_label_batch, _ in dataloader:\n",
        "            output = classifier(emb_batch)\n",
        "            loss = criterion(output, role_label_batch)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.data.mean().item())\n",
        "        print('[%d/%d] Train loss: %.3f' % (epoch+1, epochs, np.mean(losses)))\n",
        "    return classifier\n",
        "\n",
        "def train_classifier_logistic(train_dataset):\n",
        "    X, y = [], []\n",
        "    dataloader = train_dataset.get_dataloader(batch_size=1)\n",
        "    for emb_batch, role_label_batch, _ in dataloader:\n",
        "        X.append(emb_batch[0])\n",
        "        y.append(role_label_batch[0])\n",
        "    X = np.stack(X, axis=0)\n",
        "    y = np.stack(y, axis=0)\n",
        "    scaler = preprocessing.StandardScaler().fit(X)\n",
        "    X_scaled = scaler.transform(X)\n",
        "    classifier = LogisticRegression(random_state=0, max_iter=10000).fit(X_scaled, y)\n",
        "    return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4COaxeY6Fjjv",
        "outputId": "79fd470d-0265-4a9d-a131-8926d5152f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data.py\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "from utils import load_bert_outputs, load_embeddings\n",
        "\n",
        "class SimpleDataset(data.Dataset):\n",
        "    def __init__(self, dataset_name, index_name, layer_num, old_labeldict = None, pool_method = \"first\"):\n",
        "        self.layer_num = layer_num\n",
        "        self.pool_method = pool_method\n",
        "        #dataset_directory = os.path.join(\"dataset_storing\", dataset_name)\n",
        "        dataset_directory = \"/content\"\n",
        "        self.labels = json.load(open(os.path.join(dataset_directory, \"labels.json\"), \"r\"))\n",
        "        self.bert_info = pickle.load(open(os.path.join(dataset_directory, \"bert_info.pkl\"), \"rb\"))\n",
        "\n",
        "        if layer_num in [\"word_embeddings\", \"position_embeddings\"]:\n",
        "            self.bert_outputs = load_embeddings(dataset_directory, layer_num)\n",
        "        else:\n",
        "            try:\n",
        "                print(\"in try\", layer_num)\n",
        "                int_layer = int(layer_num)\n",
        "                self.bert_outputs = load_bert_outputs(dataset_directory, layer_num)\n",
        "            except:\n",
        "                print(f\"Please put a valid layer name, {layer_num} is not a layer\")\n",
        "                sys.exit(1)\n",
        "        self.index = json.load(open(os.path.join(dataset_directory, index_name + \".json\"), \"r\"))\n",
        "        print(\"Examples #\", len(self.index))\n",
        "        self.labeldict = self.get_label_dict(old_labeldict)\n",
        "        self.labelset = sorted(self.labeldict.keys(), key=lambda x: self.labeldict[x])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence_num, word_num = self.index[idx]\n",
        "        bert_start_index = self.bert_info[\"orig_to_bert_map\"][sentence_num][word_num]\n",
        "        bert_end_index = self.bert_info[\"orig_to_bert_map\"][sentence_num][word_num + 1]\n",
        "        embedding = self.get_pooled_embedding(sentence_num, bert_start_index,\n",
        "                                              bert_end_index)\n",
        "        role = self.labels[\"role\"][sentence_num][word_num]\n",
        "        role_label_idx = self.labeldict[role] if role in self.labeldict else -1\n",
        "        aux_labels = {}\n",
        "        for label_type in self.labels.keys():\n",
        "            label = self.labels[label_type][sentence_num][word_num]\n",
        "            if label == None:\n",
        "                label = \"\"\n",
        "            aux_labels[label_type] = label\n",
        "        #aux_labels[\"word_index\"] = word_num\n",
        "        return embedding, role_label_idx, aux_labels\n",
        "\n",
        "    # Make a labeldict of all of the labels in this dataset, keeping the same\n",
        "    # order for labels already in the old labeldict\n",
        "    def get_label_dict(self, old_labeldict):\n",
        "        all_labels = set()\n",
        "        for sent_i, word_i in self.index:\n",
        "            new_role = self.labels[\"role\"][sent_i][word_i]\n",
        "            if new_role is not None:\n",
        "                all_labels.add(new_role)\n",
        "        labelset = sorted(list(all_labels))\n",
        "        if old_labeldict is None:\n",
        "            curr_label = 0\n",
        "            labeldict = {}\n",
        "        else:\n",
        "            labeldict = old_labeldict\n",
        "            curr_label = len(old_labeldict)\n",
        "        for label in labelset:\n",
        "            if old_labeldict is None or label not in old_labeldict:\n",
        "                labeldict[label] = curr_label\n",
        "                curr_label += 1\n",
        "        return labeldict\n",
        "\n",
        "    def get_num_labels(self):\n",
        "        return len(self.labeldict)\n",
        "\n",
        "    def get_bert_dim(self):\n",
        "        return self.bert_outputs[0].shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def get_pooled_embedding(self, sentence_num, bert_start_index, bert_end_index):\n",
        "        bert_sentence = \\\n",
        "            self.bert_outputs[sentence_num].squeeze()\n",
        "        if self.pool_method == \"first\":\n",
        "            return bert_sentence[bert_start_index]\n",
        "        elif self.pool_method == \"average\":\n",
        "            return np.mean(\n",
        "                bert_outputs[sentence_num][self.layer_num].squeeze()\\\n",
        "                    [bert_start_index:bert_end_index])\n",
        "\n",
        "    def get_dataloader(self, batch_size=32, shuffle=True):\n",
        "      return data.DataLoader(self, batch_size=batch_size, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIVXkzDCqV0A",
        "outputId": "69fce1b9-1b26-4f60-ea50-afd47005a24f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/utils.py:200: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  if label is not \"shuffled_index\":\n",
            "args: Namespace(ud_path='/content/concat-ud.conllu', csv_file=None, bert_name='bert-base-uncased', shuffle_positions=False, single_position=-1, local_shuffle=-1)\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 113kB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 2.55MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 3.40MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 3.43MB/s]\n",
            "model.safetensors: 100% 440M/440M [00:03<00:00, 129MB/s]\n",
            "returning from get_tokens, the keys are ['sent_id', 'token', 'role', 'verb_word', 'verb_idx', 'subject_word', 'object_word', 'case', 'animacy', 'preceding_nouns', 'word_index']\n",
            "Not shuffling positions this time\n",
            "Running 10625 sentences through BERT. This takes a while\n",
            "100% 10625/10625 [23:05<00:00,  7.67it/s]\n"
          ]
        }
      ],
      "source": [
        "!python /content/create_dataset.py --ud-path /content/concat-ud.conllu --bert-name bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LVBnMoX_krl",
        "outputId": "f19d7ac7-9489-4645-8665-cae7a98383d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='concat-ud_bert-base-uncased', roles=['A', 'O'], cases=None, balance=True, only_non_prototypical=False, limit=-1)\n",
            "Culling all roles to have length 1469\n",
            "Index has length 2938\n"
          ]
        }
      ],
      "source": [
        "!python /content/create_index.py --dataset concat-ud_bert-base-uncased --roles A O --balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4lafcC2CPTT",
        "outputId": "ae1c311b-733b-4475-8a5e-5435ea530a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args Namespace(dataset_name='concat-ud_bert-base-uncased', index_name='index_balance_roles-AO', classifier_type='mlp')\n",
            "making classifier dir at classifiers/concat-ud_bert-base-uncased/index_balance_roles-AO\n",
            "There are 13 layers in this model\n",
            "Layer word_embeddings\n",
            "[Loading from disk]: 100% 10625/10625 [00:03<00:00, 3482.90it/s]\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.681\n",
            "[2/20] Train loss: 0.623\n",
            "[3/20] Train loss: 0.575\n",
            "[4/20] Train loss: 0.556\n",
            "[5/20] Train loss: 0.530\n",
            "[6/20] Train loss: 0.517\n",
            "[7/20] Train loss: 0.513\n",
            "[8/20] Train loss: 0.502\n",
            "[9/20] Train loss: 0.492\n",
            "[10/20] Train loss: 0.490\n",
            "[11/20] Train loss: 0.473\n",
            "[12/20] Train loss: 0.469\n",
            "[13/20] Train loss: 0.458\n",
            "[14/20] Train loss: 0.446\n",
            "[15/20] Train loss: 0.438\n",
            "[16/20] Train loss: 0.433\n",
            "[17/20] Train loss: 0.421\n",
            "[18/20] Train loss: 0.418\n",
            "[19/20] Train loss: 0.405\n",
            "[20/20] Train loss: 0.400\n",
            "Layer 0\n",
            "in try 0\n",
            "[Loading from disk]: 100% 10625/10625 [00:18<00:00, 586.49it/s] \n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.537\n",
            "[2/20] Train loss: 0.413\n",
            "[3/20] Train loss: 0.351\n",
            "[4/20] Train loss: 0.302\n",
            "[5/20] Train loss: 0.255\n",
            "[6/20] Train loss: 0.219\n",
            "[7/20] Train loss: 0.184\n",
            "[8/20] Train loss: 0.161\n",
            "[9/20] Train loss: 0.146\n",
            "[10/20] Train loss: 0.132\n",
            "[11/20] Train loss: 0.114\n",
            "[12/20] Train loss: 0.099\n",
            "[13/20] Train loss: 0.095\n",
            "[14/20] Train loss: 0.093\n",
            "[15/20] Train loss: 0.091\n",
            "[16/20] Train loss: 0.081\n",
            "[17/20] Train loss: 0.075\n",
            "[18/20] Train loss: 0.075\n",
            "[19/20] Train loss: 0.077\n",
            "[20/20] Train loss: 0.066\n",
            "Layer 1\n",
            "in try 1\n",
            "[Loading from disk]: 100% 10625/10625 [00:07<00:00, 1372.95it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.437\n",
            "[2/20] Train loss: 0.264\n",
            "[3/20] Train loss: 0.201\n",
            "[4/20] Train loss: 0.146\n",
            "[5/20] Train loss: 0.112\n",
            "[6/20] Train loss: 0.090\n",
            "[7/20] Train loss: 0.075\n",
            "[8/20] Train loss: 0.063\n",
            "[9/20] Train loss: 0.048\n",
            "[10/20] Train loss: 0.049\n",
            "[11/20] Train loss: 0.038\n",
            "[12/20] Train loss: 0.037\n",
            "[13/20] Train loss: 0.030\n",
            "[14/20] Train loss: 0.024\n",
            "[15/20] Train loss: 0.027\n",
            "[16/20] Train loss: 0.018\n",
            "[17/20] Train loss: 0.026\n",
            "[18/20] Train loss: 0.018\n",
            "[19/20] Train loss: 0.020\n",
            "[20/20] Train loss: 0.023\n",
            "Layer 2\n",
            "in try 2\n",
            "[Loading from disk]: 100% 10625/10625 [00:08<00:00, 1181.37it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.399\n",
            "[2/20] Train loss: 0.239\n",
            "[3/20] Train loss: 0.177\n",
            "[4/20] Train loss: 0.130\n",
            "[5/20] Train loss: 0.092\n",
            "[6/20] Train loss: 0.074\n",
            "[7/20] Train loss: 0.065\n",
            "[8/20] Train loss: 0.054\n",
            "[9/20] Train loss: 0.043\n",
            "[10/20] Train loss: 0.037\n",
            "[11/20] Train loss: 0.029\n",
            "[12/20] Train loss: 0.027\n",
            "[13/20] Train loss: 0.030\n",
            "[14/20] Train loss: 0.024\n",
            "[15/20] Train loss: 0.018\n",
            "[16/20] Train loss: 0.011\n",
            "[17/20] Train loss: 0.013\n",
            "[18/20] Train loss: 0.011\n",
            "[19/20] Train loss: 0.010\n",
            "[20/20] Train loss: 0.012\n",
            "Layer 3\n",
            "in try 3\n",
            "[Loading from disk]: 100% 10625/10625 [00:03<00:00, 2689.13it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.317\n",
            "[2/20] Train loss: 0.173\n",
            "[3/20] Train loss: 0.130\n",
            "[4/20] Train loss: 0.097\n",
            "[5/20] Train loss: 0.074\n",
            "[6/20] Train loss: 0.056\n",
            "[7/20] Train loss: 0.044\n",
            "[8/20] Train loss: 0.033\n",
            "[9/20] Train loss: 0.035\n",
            "[10/20] Train loss: 0.032\n",
            "[11/20] Train loss: 0.024\n",
            "[12/20] Train loss: 0.018\n",
            "[13/20] Train loss: 0.016\n",
            "[14/20] Train loss: 0.014\n",
            "[15/20] Train loss: 0.017\n",
            "[16/20] Train loss: 0.011\n",
            "[17/20] Train loss: 0.008\n",
            "[18/20] Train loss: 0.014\n",
            "[19/20] Train loss: 0.016\n",
            "[20/20] Train loss: 0.014\n",
            "Layer 4\n",
            "in try 4\n",
            "[Loading from disk]: 100% 10625/10625 [00:03<00:00, 2742.31it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.260\n",
            "[2/20] Train loss: 0.113\n",
            "[3/20] Train loss: 0.078\n",
            "[4/20] Train loss: 0.056\n",
            "[5/20] Train loss: 0.043\n",
            "[6/20] Train loss: 0.034\n",
            "[7/20] Train loss: 0.027\n",
            "[8/20] Train loss: 0.019\n",
            "[9/20] Train loss: 0.019\n",
            "[10/20] Train loss: 0.013\n",
            "[11/20] Train loss: 0.011\n",
            "[12/20] Train loss: 0.014\n",
            "[13/20] Train loss: 0.012\n",
            "[14/20] Train loss: 0.010\n",
            "[15/20] Train loss: 0.009\n",
            "[16/20] Train loss: 0.010\n",
            "[17/20] Train loss: 0.008\n",
            "[18/20] Train loss: 0.010\n",
            "[19/20] Train loss: 0.007\n",
            "[20/20] Train loss: 0.008\n",
            "Layer 5\n",
            "in try 5\n",
            "[Loading from disk]: 100% 10625/10625 [00:04<00:00, 2305.89it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.237\n",
            "[2/20] Train loss: 0.096\n",
            "[3/20] Train loss: 0.070\n",
            "[4/20] Train loss: 0.043\n",
            "[5/20] Train loss: 0.031\n",
            "[6/20] Train loss: 0.023\n",
            "[7/20] Train loss: 0.016\n",
            "[8/20] Train loss: 0.015\n",
            "[9/20] Train loss: 0.012\n",
            "[10/20] Train loss: 0.007\n",
            "[11/20] Train loss: 0.010\n",
            "[12/20] Train loss: 0.009\n",
            "[13/20] Train loss: 0.008\n",
            "[14/20] Train loss: 0.006\n",
            "[15/20] Train loss: 0.008\n",
            "[16/20] Train loss: 0.010\n",
            "[17/20] Train loss: 0.010\n",
            "[18/20] Train loss: 0.009\n",
            "[19/20] Train loss: 0.008\n",
            "[20/20] Train loss: 0.006\n",
            "Layer 6\n",
            "in try 6\n",
            "[Loading from disk]: 100% 10625/10625 [00:03<00:00, 3067.75it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.212\n",
            "[2/20] Train loss: 0.066\n",
            "[3/20] Train loss: 0.041\n",
            "[4/20] Train loss: 0.033\n",
            "[5/20] Train loss: 0.020\n",
            "[6/20] Train loss: 0.015\n",
            "[7/20] Train loss: 0.012\n",
            "[8/20] Train loss: 0.012\n",
            "[9/20] Train loss: 0.009\n",
            "[10/20] Train loss: 0.009\n",
            "[11/20] Train loss: 0.009\n",
            "[12/20] Train loss: 0.010\n",
            "[13/20] Train loss: 0.008\n",
            "[14/20] Train loss: 0.007\n",
            "[15/20] Train loss: 0.008\n",
            "[16/20] Train loss: 0.009\n",
            "[17/20] Train loss: 0.011\n",
            "[18/20] Train loss: 0.007\n",
            "[19/20] Train loss: 0.009\n",
            "[20/20] Train loss: 0.008\n",
            "Layer 7\n",
            "in try 7\n",
            "[Loading from disk]: 100% 10625/10625 [00:09<00:00, 1173.09it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.200\n",
            "[2/20] Train loss: 0.064\n",
            "[3/20] Train loss: 0.041\n",
            "[4/20] Train loss: 0.030\n",
            "[5/20] Train loss: 0.019\n",
            "[6/20] Train loss: 0.016\n",
            "[7/20] Train loss: 0.017\n",
            "[8/20] Train loss: 0.013\n",
            "[9/20] Train loss: 0.011\n",
            "[10/20] Train loss: 0.008\n",
            "[11/20] Train loss: 0.008\n",
            "[12/20] Train loss: 0.010\n",
            "[13/20] Train loss: 0.007\n",
            "[14/20] Train loss: 0.008\n",
            "[15/20] Train loss: 0.006\n",
            "[16/20] Train loss: 0.010\n",
            "[17/20] Train loss: 0.009\n",
            "[18/20] Train loss: 0.007\n",
            "[19/20] Train loss: 0.008\n",
            "[20/20] Train loss: 0.005\n",
            "Layer 8\n",
            "in try 8\n",
            "[Loading from disk]: 100% 10625/10625 [00:06<00:00, 1653.28it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.197\n",
            "[2/20] Train loss: 0.053\n",
            "[3/20] Train loss: 0.034\n",
            "[4/20] Train loss: 0.021\n",
            "[5/20] Train loss: 0.015\n",
            "[6/20] Train loss: 0.011\n",
            "[7/20] Train loss: 0.009\n",
            "[8/20] Train loss: 0.008\n",
            "[9/20] Train loss: 0.011\n",
            "[10/20] Train loss: 0.008\n",
            "[11/20] Train loss: 0.009\n",
            "[12/20] Train loss: 0.009\n",
            "[13/20] Train loss: 0.009\n",
            "[14/20] Train loss: 0.005\n",
            "[15/20] Train loss: 0.009\n",
            "[16/20] Train loss: 0.010\n",
            "[17/20] Train loss: 0.008\n",
            "[18/20] Train loss: 0.011\n",
            "[19/20] Train loss: 0.011\n",
            "[20/20] Train loss: 0.009\n",
            "Layer 9\n",
            "in try 9\n",
            "[Loading from disk]: 100% 10625/10625 [00:05<00:00, 2096.22it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.221\n",
            "[2/20] Train loss: 0.069\n",
            "[3/20] Train loss: 0.040\n",
            "[4/20] Train loss: 0.027\n",
            "[5/20] Train loss: 0.021\n",
            "[6/20] Train loss: 0.019\n",
            "[7/20] Train loss: 0.010\n",
            "[8/20] Train loss: 0.011\n",
            "[9/20] Train loss: 0.011\n",
            "[10/20] Train loss: 0.010\n",
            "[11/20] Train loss: 0.008\n",
            "[12/20] Train loss: 0.008\n",
            "[13/20] Train loss: 0.009\n",
            "[14/20] Train loss: 0.008\n",
            "[15/20] Train loss: 0.010\n",
            "[16/20] Train loss: 0.009\n",
            "[17/20] Train loss: 0.005\n",
            "[18/20] Train loss: 0.008\n",
            "[19/20] Train loss: 0.006\n",
            "[20/20] Train loss: 0.009\n",
            "Layer 10\n",
            "in try 10\n",
            "[Loading from disk]: 100% 10625/10625 [00:03<00:00, 2909.37it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.243\n",
            "[2/20] Train loss: 0.085\n",
            "[3/20] Train loss: 0.051\n",
            "[4/20] Train loss: 0.044\n",
            "[5/20] Train loss: 0.030\n",
            "[6/20] Train loss: 0.022\n",
            "[7/20] Train loss: 0.014\n",
            "[8/20] Train loss: 0.015\n",
            "[9/20] Train loss: 0.011\n",
            "[10/20] Train loss: 0.009\n",
            "[11/20] Train loss: 0.009\n",
            "[12/20] Train loss: 0.008\n",
            "[13/20] Train loss: 0.011\n",
            "[14/20] Train loss: 0.010\n",
            "[15/20] Train loss: 0.006\n",
            "[16/20] Train loss: 0.006\n",
            "[17/20] Train loss: 0.009\n",
            "[18/20] Train loss: 0.008\n",
            "[19/20] Train loss: 0.009\n",
            "[20/20] Train loss: 0.009\n",
            "Layer 11\n",
            "in try 11\n",
            "[Loading from disk]: 100% 10625/10625 [00:03<00:00, 2776.36it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.273\n",
            "[2/20] Train loss: 0.107\n",
            "[3/20] Train loss: 0.066\n",
            "[4/20] Train loss: 0.050\n",
            "[5/20] Train loss: 0.037\n",
            "[6/20] Train loss: 0.027\n",
            "[7/20] Train loss: 0.021\n",
            "[8/20] Train loss: 0.022\n",
            "[9/20] Train loss: 0.016\n",
            "[10/20] Train loss: 0.014\n",
            "[11/20] Train loss: 0.011\n",
            "[12/20] Train loss: 0.009\n",
            "[13/20] Train loss: 0.009\n",
            "[14/20] Train loss: 0.010\n",
            "[15/20] Train loss: 0.007\n",
            "[16/20] Train loss: 0.007\n",
            "[17/20] Train loss: 0.007\n",
            "[18/20] Train loss: 0.008\n",
            "[19/20] Train loss: 0.008\n",
            "[20/20] Train loss: 0.006\n",
            "Layer 12\n",
            "in try 12\n",
            "[Loading from disk]: 100% 10625/10625 [00:03<00:00, 2859.62it/s]\n",
            "Loaded 10624 sentences from disk.\n",
            "Examples # 2938\n",
            "[1/20] Train loss: 0.343\n",
            "[2/20] Train loss: 0.160\n",
            "[3/20] Train loss: 0.110\n",
            "[4/20] Train loss: 0.081\n",
            "[5/20] Train loss: 0.063\n",
            "[6/20] Train loss: 0.051\n",
            "[7/20] Train loss: 0.036\n",
            "[8/20] Train loss: 0.027\n",
            "[9/20] Train loss: 0.027\n",
            "[10/20] Train loss: 0.018\n",
            "[11/20] Train loss: 0.015\n",
            "[12/20] Train loss: 0.015\n",
            "[13/20] Train loss: 0.011\n",
            "[14/20] Train loss: 0.013\n",
            "[15/20] Train loss: 0.014\n",
            "[16/20] Train loss: 0.008\n",
            "[17/20] Train loss: 0.011\n",
            "[18/20] Train loss: 0.010\n",
            "[19/20] Train loss: 0.009\n",
            "[20/20] Train loss: 0.013\n",
            "Layer 13\n",
            "in try 13\n",
            "[Loading from disk]:   0% 0/10625 [00:00<?, ?it/s]\n",
            "Please put a valid layer name, 13 is not a layer\n"
          ]
        }
      ],
      "source": [
        "!python train_classifiers.py --dataset concat-ud_bert-base-uncased --index-name index_balance_roles-AO --classifier-type mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbBuKxUnHa6j"
      },
      "outputs": [],
      "source": [
        "#Test on original and argument-swapped versions of our sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCgMK_qbLibO",
        "outputId": "7aa5358f-61fe-4ba7-9ed9-dc014f387363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting eval_classifiers.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile eval_classifiers.py\n",
        "\n",
        "import argparse\n",
        "from collections import defaultdict\n",
        "import datetime\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "from data import SimpleDataset\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--train-dataset', type=str)\n",
        "    parser.add_argument('--train-index', type=str)\n",
        "    parser.add_argument(\"--classifier-type\", type=str, default=\"mlp\")\n",
        "    parser.add_argument('--eval-dataset', type=str)\n",
        "    parser.add_argument('--eval-index', type=str)\n",
        "    args = parser.parse_args()\n",
        "    print(\"args\", args)\n",
        "\n",
        "    eval_classifiers(args)\n",
        "\n",
        "\n",
        "def eval_classifiers(args):\n",
        "    #classifier_dir = os.path.join(\"classifiers\", args.train_dataset, args.train_index)\n",
        "    classifier_dir = \"/content/classifiers/concat-ud_bert-base-uncased/index_balance_roles-AO\"\n",
        "    print(\"Evaluating classifiers at\", classifier_dir)\n",
        "    #layers = [\"position_embeddings\", \"word_embeddings\"] +  [str(i) for i in range(13)]\n",
        "    layers = [\"word_embeddings\"] +  [str(i) for i in range(13)]\n",
        "    logistic = args.classifier_type == \"logistic\"\n",
        "    results = defaultdict(list)\n",
        "    for layer in layers:\n",
        "        classifier, labelset, labeldict = pickle.load(\n",
        "            open(os.path.join(classifier_dir, f\"{args.classifier_type}_layer-{layer}\"), \"rb\"))\n",
        "        print(\"Classifier labeldict\", labeldict)\n",
        "        A_index = labeldict[\"A\"]\n",
        "        eval_dataset = SimpleDataset(args.eval_dataset, args.eval_index, layer, old_labeldict = labeldict)\n",
        "        dataloader = eval_dataset.get_dataloader(batch_size=1)\n",
        "        classifier.eval()\n",
        "        for embedding, role, other_labels in dataloader:\n",
        "            if args.classifier_type == \"logistic\":\n",
        "                probs = classifier.predict_proba(torch.Tensor(embedding))[0]\n",
        "                A_prob = probs[A_index]\n",
        "            elif args.classifier_type == \"mlp\":\n",
        "                output = classifier(torch.Tensor(embedding))\n",
        "                probs = torch.softmax(output, 1)\n",
        "                A_prob = probs[:,A_index][0].item()\n",
        "            for label in other_labels.keys():\n",
        "                val = other_labels[label][0]\n",
        "                if type(val) == torch.Tensor:\n",
        "                    val = val.item()\n",
        "                results[label].append(val)\n",
        "            results[\"layer\"].append(layer)\n",
        "            results[\"probability_A\"].append(A_prob)\n",
        "    df = pd.DataFrame(results)\n",
        "    date_string = datetime.datetime.now().strftime(\"%m%d%Y\")\n",
        "    #output_file = f\"{date_string}_train-{args.train_dataset}-{args.train_index}_eval-{args.eval_dataset}-{args.eval_index}.csv\"\n",
        "    #df.to_csv(open(os.path.join(\"results\", \"long_names\", output_file), \"w\"))\n",
        "    df.to_csv(\"/content/results-switched.csv\")\n",
        "\n",
        "\n",
        "\n",
        "def add_classifier_predictions(labels_file, vecs_file, classifier, label_dict, layer, classifier_type):\n",
        "    vecs = h5py.File(vecs_file, \"r\")[f\"bert_layer_{layer}\"]\n",
        "    labels = json.load(open(labels_file, \"r\"))\n",
        "    print(\"after loading\", labels.keys())\n",
        "    length = len(vecs)\n",
        "    labels[f\"probA_{classifier_type}_layer_{layer}\"] = [0]*length\n",
        "    A_index = label_dict[\"A\"]\n",
        "    for i in range(length):\n",
        "        if classifier_type == \"logistic\":\n",
        "            probs = classifier.predict_proba(torch.Tensor(vecs[i].astype(np.float32)).unsqueeze(0))[0]\n",
        "            A_prob = probs[A_index]\n",
        "        elif classifier_type == \"mlp\":\n",
        "            output = classifier(torch.Tensor(vecs[i].astype(np.float32)).unsqueeze(0))\n",
        "            probs = torch.softmax(output, 1)\n",
        "            A_prob = probs[:,A_index][0].item()\n",
        "        labels[f\"probA_{classifier_type}_layer_{layer}\"][i] = A_prob\n",
        "    print(\"before saving\", labels.keys())\n",
        "    json.dump(labels, open(labels_file, \"w\"))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhcPIzM9TA1W",
        "outputId": "2e970fe2-4a6c-44f4-f30c-279193e50de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args: Namespace(ud_path=None, csv_file='/content/object-subject-original.csv', bert_name='bert-base-uncased', shuffle_positions=False, single_position=-1, local_shuffle=-1)\n",
            "Not shuffling positions this time\n",
            "Running 53 sentences through BERT. This takes a while\n",
            "100% 53/53 [00:07<00:00,  7.57it/s]\n"
          ]
        }
      ],
      "source": [
        "!python create_dataset.py --csv-file  /content/object-subject-original.csv --bert-name bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LlJrXLhIDgG",
        "outputId": "fc27df12-7ee5-4228-830a-ed1ddd0e07be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='object-subject-original_bert-base-uncased', roles=None, cases=None, balance=False, only_non_prototypical=False, limit=-1)\n",
            "Index has length 106\n"
          ]
        }
      ],
      "source": [
        "!python create_index.py --dataset object-subject-original_bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pyQJ44ILf58",
        "outputId": "3df2d63a-2f5a-4613-80dd-3e3e2c68d31c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args Namespace(train_dataset='concat-ud_bert-base-uncased', train_index='index_balance_roles-AO', classifier_type='mlp', eval_dataset='object-subject-original_bert-base-uncased', eval_index='index')\n",
            "Evaluating classifiers at /content/classifiers/concat-ud_bert-base-uncased/index_balance_roles-AO\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 3994.86it/s]\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 0\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 4657.80it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 1\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 5708.15it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 2\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 5405.29it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 3\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 4493.14it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 4\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 5126.21it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 5\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 3908.26it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 6\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5084.12it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 7\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 4867.49it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 8\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5676.66it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 9\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5381.22it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 10\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 6183.54it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 11\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 4291.97it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 12\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5818.41it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n"
          ]
        }
      ],
      "source": [
        "!python eval_classifiers.py --train-dataset concat-ud_bert-base-uncased --train-index index_balance_roles-AO --classifier-type mlp --eval-dataset object-subject-original_bert-base-uncased --eval-index index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python create_dataset.py --csv-file  /content/object-subject-switched.csv --bert-name bert-base-uncased\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kMwuio6gxrg",
        "outputId": "a50b78a3-a671-45f8-fcb2-731c8146ce8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args: Namespace(ud_path=None, csv_file='/content/object-subject-switched.csv', bert_name='bert-base-uncased', shuffle_positions=False, single_position=-1, local_shuffle=-1)\n",
            "Not shuffling positions this time\n",
            "Running 53 sentences through BERT. This takes a while\n",
            "100% 53/53 [00:07<00:00,  7.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python create_index.py --dataset object-subject-switched_bert-base-uncased"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gxWGNwPhbOi",
        "outputId": "acfd9a90-e9e3-4a1d-a0e4-cb213db8783e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='object-subject-switched_bert-base-uncased', roles=None, cases=None, balance=False, only_non_prototypical=False, limit=-1)\n",
            "Index has length 106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adjust csv name in eval_classifiers before doing this\n",
        "!python eval_classifiers.py --train-dataset concat-ud_bert-base-uncased --train-index index_balance_roles-AO --classifier-type mlp --eval-dataset object-subject-switched_bert-base-uncased --eval-index index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLGLxA9GhdNJ",
        "outputId": "9873d648-e6f0-4fe3-c964-fbe5a0e6a9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args Namespace(train_dataset='concat-ud_bert-base-uncased', train_index='index_balance_roles-AO', classifier_type='mlp', eval_dataset='object-subject-switched_bert-base-uncased', eval_index='index')\n",
            "Evaluating classifiers at /content/classifiers/concat-ud_bert-base-uncased/index_balance_roles-AO\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 4328.07it/s]\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 0\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 4955.71it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 1\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 5650.26it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 2\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 5850.41it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 3\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 5717.99it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 4\n",
            "\r[Loading from disk]:   0% 0/53 [00:00<?, ?it/s]\r[Loading from disk]: 100% 53/53 [00:00<00:00, 5816.89it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 5\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5255.77it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 6\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5490.60it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 7\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5548.99it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 8\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5812.63it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 9\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 6074.05it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 10\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5452.89it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 11\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 5936.97it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n",
            "Classifier labeldict {'A': 0, 'O': 1}\n",
            "in try 12\n",
            "[Loading from disk]: 100% 53/53 [00:00<00:00, 4141.56it/s]\n",
            "Loaded 52 sentences from disk.\n",
            "Examples # 106\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}